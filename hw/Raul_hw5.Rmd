---
title: "Homework 5"
author: "Raul Torres Aragon"
date: "10/13/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#load("hw5_en.Rdata")
```

# Q1  
Suppose $V \sim \mathcal{X}^2_n$. Then $V$ has the same distribution as 
$\sum_{i=1}^n X_i^2$ where the $X_i$ are independent and $X_i \sim N(0,1)$, 
$i = \dots , n$. It follows from the central limit theorem that  
$$
T_n = \frac{V-n}{\sqrt{2n}} \sim N(0,1)
$$  
  
## (a)  
Show that the Edgeworth approximation to the distribution of $T_n$ to the order of 
$\mathcal{O}(1/n)$ is given by  
$$
F_n(x) = P(T_n \leq x) = \Phi(x) - \phi(x) 
\bigg\{ 
\frac{\sqrt{2}}{3\sqrt{n}} H_2(x) + \frac{1}{2n}H_3(x) + \frac{1}{9n}H_5(x) + \mathcal{O}\bigg( \frac{1}{n} \bigg)
\bigg\}
$$  
where $H_i(x)$, $i = 0,1,2,\dots$, are Hermite polynomials.  

We know that for a variable $V \sim \mathcal{X}^2[n]$ variable, the moments are:  
$$
\begin{aligned}
E(V) &= n \\
E(V^2) &= n(n+2) \\
E(V^3) &= n(n+2)(n+4) \\
E(V^4) &= n(n+2)(n+4)(n+6)
\end{aligned}
$$  
Furthermore, by the Berry-Essen theorem $|F_n(x)-\Phi(x)|=\mathcal{O}[1/\sqrt(n)]$ uniformly in $x$ if $X$ has three moments. So,     
$$
\begin{aligned}
F_n(x) &= P(Z_n\leq x)  \\
&= P\bigg( \frac{\sqrt{n}(\bar{X}-\mu)}{\sigma} \leq x\bigg) \\
&= \Phi(x)-\phi(x)\bigg\{ \frac{1}{6} \gamma_1 H_2(x) + 
\frac{1}{24} \gamma_2 H_3(x) + 
\frac{1}{72} \gamma_1^2+ \mathcal{O}(1/n)  \bigg\}
\end{aligned}
$$  
where $H_i(x)$ is the $i^{th}$ Hermite polynomial, and $\gamma_1 = E(\frac{X-\mu}{\sigma})^3$, and $\gamma_2 = E(\frac{X-\mu}{\sigma})^4-3$.

Now, notice that for $V$:  

$$
\begin{aligned}
\gamma_1 &= E\bigg(\frac{V-n}{\sqrt{2n}}\bigg)^3 \\
&= \frac{1}{(\sqrt{2n})^3} E(V^3-n^3-3V^2n+3n^2V) \\
&= \frac{1}{2n\sqrt{2n}} n(n+2)(n+4)-n^3-3n^2(n+2)+3n^3 \\
&= \frac{8n}{2n\sqrt{2n}} \\ 
&= \frac{4}{\sqrt{2n}} \times \frac{\sqrt{2}}{\sqrt{2}} \\
&= \frac{2\sqrt{2}}{\sqrt{n}}
\end{aligned}
$$  

And we also get  
$$
\begin{aligned}
\gamma_1 &= E\bigg(\frac{V-n}{\sqrt{2n}}\bigg)^4-3 \\
&= \frac{1}{(\sqrt{2n})^4} E(V^4-4n^3+6n^2V^2-4nV^3+n^4) -3\\
&= n(n+2)(n+4)(n+6)-4n^4+6n^4(n+2)-4n^2(n+2)(n+4)+n^4-3 \\
&= \frac{12n^2+48n}{4n^2}-3 \\
&= 3-3+12/n = 12/n
\end{aligned}
$$  
  
So, in our case, the standardized $V$ is $T_n = \frac{(V-n)}{\sqrt{2n}}$. Thus, the Edgeworth expansion is:  
$$
\begin{aligned}
V_n &= P(Z_n\leq x) \\
&= \Phi(x)-\phi(x)\bigg\{ \frac{1}{6} \gamma_1 H_2(x) + 
\frac{1}{24} \gamma_2 H_3(x) + 
\frac{1}{72} \gamma_1^2+ \mathcal{O}(1/n)  \bigg\} \\
&= \Phi(x)-\phi(x)\bigg\{ \frac{1}{6} \frac{2\sqrt{2}}{\sqrt{n}} H_2(x) + 
\frac{1}{24} \frac{12}{n} H_3(x) + 
\frac{1}{72} (\frac{2\sqrt{2}}{\sqrt{n}})^2+ \mathcal{O}(1/n)  \bigg\} \\
&= \Phi(x) - \phi(x) 
\bigg\{ 
\frac{\sqrt{2}}{3\sqrt{n}} H_2(x) + \frac{1}{2n}H_3(x) + \frac{1}{9n}H_5(x) + \mathcal{O}(1/n) \bigg\}
\end{aligned}
$$

## (b)  
Complete the table below that gives the approximation above together with the exact distribution and the normal approximation when $n=10$. Make sure all the entries are to the fourth decimal place.  

```{r, fig.align='center', out.width="64%", echo=FALSE}
knitr::include_graphics("hw/hw5_table1.png")
```  

```{r}
n <- 10
x <- c(-2.04,-1.95,-1.91,-1.75,-1.66, -1.51, -1.35,-1.15,-0.85,-0.61,-0.38,-0.15,
       0.11, 0.4, 0.77, 1.34, 1.86, 2.34, 2.95, 3.4, 4.38, 4.79, 5.72)

Exact <- function(x,n=10) { 
   #T_n = (V-n)/sqrt(2n) -> V=T_n*(sqrt(2n))+n
   pchisq(x*(sqrt(2*n))+n, n) 
}

Edgesworth <- function(x,n=10) { 
   H2 <-  x^2-1 
   H3 <-  x^3-3*x 
   H5 <-  x^5-10*x^3+15*x  
   g1 <- sqrt(2)/(3*sqrt(n))
   g2 <- sqrt(1/(2*n))
   g12<- g1^2
   pnorm(x) - dnorm(x) * (g1*H2 + g2*H3 + g12*H5)
}

tab1 <- data.frame(X=x, Exact=Exact(x), EA=Edgesworth(x), "NA"=pnorm(x))

knitr::kable(tab1 |> round(4))
```

# Q2  
Let $X_1, \dots, X_n$ be a random sample from a normal distribution with mean $\mu$ and variance $\sigma^2$. Show that the saddle point approximation to the density of $S_n = \sim_{i=1}^n X_i$ is exact.  

By the properties of Normal random variables, the sum of $n$ independent and identically distributed normal variables is $N(n\mu, n\sigma^2)$.  

So we have to show that saddle point approximation to $f_{S_n} = (\sqrt{2\pi\sigma^2})^{-1}\exp\{ -\frac{1}{2} (\frac{s-n\mu}{n\sigma})^2\}$.  

Now notice that the moment generating (MGF) of $S_n$ is $\exp\{ n\mu t + n\sigma^2t^2/2\}$, and since if two R.V.s have the same MGF, then they share the same distribution (up to a point of convergence), if follows that if the saddle point approximation results in the MGF of $S_n$, then the saddle point approximation to the density of $S_n$ would be exact.  

So, we start with the saddle point approximation's cumulants
$$
\begin{aligned}
K(t) &= \log[E(e^{tX})] \\
K_{S_n} &= \log[E(e^{tS_n})] \\
&= \log[\text{MGF}_{S_n}] \\
&= \log\bigg[\exp\{ n\mu t + n\sigma^2t^2/2\}\bigg] \\
&= n\mu t + n\sigma^2t^2/2 \\
K'(t) &= n\mu + n\sigma^2t \\
K''(t) &= n\sigma^2 \\
\hat{t} &= \frac{X-n\mu}{n\sigma^2}
\end{aligned}
$$  
Denote the expansions approximation to $f_{S_n}$ as $\hat{f}_{S_n}$, then  
$$
\begin{aligned}
\hat{f}_{S_n} &= (\sqrt{2 \pi K''(\hat{t})})^{-1}\exp\bigg[ K_{S_n}(\hat{t})-X\hat{t}\bigg] \\
&= (\sqrt{2 \pi n\sigma^2})^{-1}\exp\bigg[ 
n\mu \frac{X-n\mu}{n\sigma^2} + 
n\sigma^2\frac{(X-n\mu)^2}{2n\sigma^2}-X\frac{X-n\mu}{n\sigma^2}
\bigg] \\
&= (\sqrt{2 \pi n\sigma^2})^{-1}\exp\bigg[ 
\frac{\mu X-n\mu^2}{\sigma^2} + \frac{(\mu X-n\mu)^2}{2n\sigma^2} - \frac{(X^2-n\mu X)}{n\sigma^2}
\bigg] \\
&= (\sqrt{2 \pi n\sigma^2})^{-1}\exp\bigg[ 
\frac{X-n\mu}{n\sigma^2}(n\mu +\frac{(X-n\mu)}{2}-X)
\bigg] \\
&= (\sqrt{2 \pi n\sigma^2})^{-1}\exp\bigg[ 
\frac{X-n\mu}{n\sigma^2}(n\mu +\frac{(X-n\mu)}{2}-X)
\bigg] \\
&= (\sqrt{2 \pi n\sigma^2})^{-1}\exp\bigg[ 
-\frac{1}{2}\frac{(X-n\mu)^2}{n\sigma^2}
\bigg] \\
&= (\sqrt{2 \pi n\sigma^2})^{-1}\exp\bigg[ 
-\frac{1}{2}\bigg( \frac{X-n\mu}{n\sigma} \bigg)^2
\bigg] \\
&= f_{S_n}
\end{aligned}
$$

